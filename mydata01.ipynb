{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "import backbones.cifar as models\n",
    "from datasets import MNIST\n",
    "from Utils import adjust_learning_rate, progress_bar, Logger, mkdir_p, Evaluation\n",
    "from openmax import compute_train_score_and_mavs_and_dists,fit_weibull,openmax\n",
    "from Modelbuilder import Network\n",
    "from Plotter import plot_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = sorted(name for name in models.__dict__\n",
    "    if not name.startswith(\"__\")\n",
    "    and callable(models.__dict__[name]))\n",
    "os.environ[\"HDF5_USE_FILE_LOCKING\"] = \"FALSE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#准备数据集\n",
    "class MyDataset(Dataset):\n",
    "   # TensorDataset继承Dataset, 重载了__init__, __getitem__, __len__\n",
    "   # 能够通过index得到数据集的数据，能够通过len得到数据集大小\n",
    "\n",
    "   def __init__(self, data_tensor, target_tensor):\n",
    "       self.data_tensor = data_tensor\n",
    "       self.target_tensor = target_tensor\n",
    "\n",
    "   def __getitem__(self, index):\n",
    "       return self.data_tensor[index], self.target_tensor[index]\n",
    "\n",
    "   def __len__(self):\n",
    "       return self.data_tensor.size(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch = 'MyNet'\n",
    "train_class_num = 2\n",
    "embed_dim = 2\n",
    "resume = ''\n",
    "lr = 0.00001\n",
    "evaluate = False\n",
    "es = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint\n",
    "checkpoint = 'checkpoints_ipynb/mnist/' + arch\n",
    "if not os.path.isdir(checkpoint):\n",
    "    mkdir_p(checkpoint)\n",
    "\n",
    "# folder to save figures\n",
    "plotfolder = 'checkpoints_ipynb/mnist/' + arch + '/plotter'\n",
    "if not os.path.isdir(plotfolder):\n",
    "    mkdir_p(plotfolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n"
     ]
    }
   ],
   "source": [
    "print('==> Preparing data..')\n",
    "train_url = \"E:\\exercise_4\\Kmeans/t_n_fequence_feature0320/n_fequence_feature_ -label.csv\" \n",
    "names = ['feature0','feature1','feature2','feature3','feature4','feature5',\n",
    "'feature6','feature7','feature8','feature9','feature10','feature11',\n",
    "'feature12','feature13','feature14','feature15',\n",
    "'feature16','feature17','class'] \n",
    "train_dataset = pd.read_csv(train_url, names=names)\n",
    "train_array = train_dataset.values\n",
    "x_train = train_array[:,0:18]\n",
    "y_train = train_array[:,18]\n",
    "\n",
    "test_url = \"E:\\exercise_4\\Kmeans/t_n_fequence_feature0320/s_n_fequence_feature_-label.csv\" \n",
    "names = ['feature0','feature1','feature2','feature3','feature4','feature5',\n",
    "'feature6','feature7','feature8','feature9','feature10','feature11',\n",
    "'feature12','feature13','feature14','feature15',\n",
    "'feature16','feature17','class'] \n",
    "test_dataset = pd.read_csv(test_url, names=names)\n",
    "test_array = test_dataset.values\n",
    "x_test = test_array[:,0:18]\n",
    "y_test = test_array[:,18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor_data[0]:  (tensor([-1.2611e-03, -5.0107e-03, -5.5262e-04, -4.3407e-03, -5.1200e-05,\n",
      "        -3.4819e-03, -4.9054e+00,  3.2057e+00,  1.0536e+01,  2.0784e+00,\n",
      "         1.1622e+01,  3.8205e+00,  1.1497e+02,  3.2958e+01,  7.7813e+01,\n",
      "         3.1822e+04,  3.2292e+00,  1.3294e+01]), 1)\n",
      "len(tensor_data):  54\n",
      "tensor_data[0]:  (tensor([-4.0345e-04, -3.9042e-03,  2.8506e-02,  1.5831e-02,  3.8812e-02,\n",
      "         3.3401e-02,  2.1563e-01,  3.2291e+00,  1.0930e+01,  7.2528e+00,\n",
      "         1.0932e+01,  7.9391e+00,  8.8870e+01,  6.6000e+01,  1.3937e+02,\n",
      "         5.5205e+04,  2.0221e+00,  6.5261e+00]), tensor(2))\n",
      "len(tensor_data):  36\n"
     ]
    }
   ],
   "source": [
    "# 生成数据\n",
    "# 训练数据\n",
    "x_train_all = np.array(x_train).astype(np.float32)\n",
    "train_dataset_X = torch.from_numpy(x_train_all)\n",
    "train_dataset_Y = np.array(y_train).astype(np.int64)\n",
    "\n",
    "trainset = MyDataset(train_dataset_X, train_dataset_Y)  # 将数据封装成Dataset\n",
    "print('tensor_data[0]: ', trainset[0])\n",
    "print('len(tensor_data): ', len(trainset))\n",
    "\n",
    "# 测试数据\n",
    "x_test_all = np.array(x_test).astype(np.float32)\n",
    "test_dataset_X = torch.from_numpy(x_test_all)\n",
    "test_dataset_Y = torch.tensor(y_test.astype(float)).long()\n",
    "\n",
    "testset = MyDataset(test_dataset_X, test_dataset_Y)  # 将数据封装成Dataset\n",
    "print('tensor_data[0]: ', testset[0])\n",
    "print('len(tensor_data): ', len(testset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.1665e-03,  6.6921e-03,  3.8906e-02,  3.2917e-02,  5.2492e-02,\n",
      "          5.5909e-02, -3.0518e-01,  3.6399e+00,  2.8547e+00,  3.5485e+00,\n",
      "          2.8709e+00,  5.0834e+00,  9.6102e+01,  4.4271e+01,  6.4052e+01,\n",
      "          2.5683e+04,  3.6889e+00,  1.6853e+01]])\n",
      "tensor([1])\n"
     ]
    }
   ],
   "source": [
    "trainloader = DataLoader(trainset, batch_size=1, shuffle=True, num_workers=0)\n",
    "testloader = DataLoader(testset, batch_size=1, shuffle=True, num_workers=0)\n",
    "for data, target in trainloader: \n",
    "    ad = data\n",
    "    ta = target\n",
    "print(ad)\n",
    "print(ta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "建立模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network(\n",
      "  (backbone): MyNet(\n",
      "    (conv1_1): Conv2d(1, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (prelu1_1): PReLU(num_parameters=1)\n",
      "    (conv1_2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (prelu1_2): PReLU(num_parameters=1)\n",
      "    (conv2_1): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (prelu2_1): PReLU(num_parameters=1)\n",
      "    (conv2_2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (prelu2_2): PReLU(num_parameters=1)\n",
      "    (conv3_1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (prelu3_1): PReLU(num_parameters=1)\n",
      "    (conv3_2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (prelu3_2): PReLU(num_parameters=1)\n",
      "  )\n",
      "  (embeddingLayer): Sequential(\n",
      "    (0): Linear(in_features=2304, out_features=2, bias=True)\n",
      "    (1): PReLU(num_parameters=1)\n",
      "  )\n",
      "  (classifier): Linear(in_features=2, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = Network(backbone=arch, num_classes=train_class_num, embed_dim=embed_dim)\n",
    "fea_dim = net.classifier.in_features\n",
    "net = net.to(device)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "if resume:\n",
    "# Load checkpoint.\n",
    "    if os.path.isfile(resume):\n",
    "        print('==> Resuming from checkpoint..')\n",
    "        checkpoint = torch.load(resume)\n",
    "        net.load_state_dict(checkpoint['net'])\n",
    "        # best_acc = checkpoint['acc']\n",
    "        # print(\"BEST_ACCURACY: \"+str(best_acc))\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        logger = Logger(os.path.join(checkpoint, 'log.txt'), resume=True)\n",
    "    else:\n",
    "        print(\"=> no checkpoint found at '{}'\".format(resume))\n",
    "else:\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'))\n",
    "    logger.set_names(['Epoch', 'Learning Rate', 'Train Loss','Train Acc.', 'Test Loss', 'Test Acc.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def train(net,trainloader,optimizer,criterion,device):\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs = inputs.reshape(1,1,1,18)\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        _, outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "            % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "    return train_loss/(batch_idx+1), correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(net, acc, epoch, path):\n",
    "    print('Saving..')\n",
    "    state = {\n",
    "        'net': net.state_dict(),\n",
    "        'testacc': acc,\n",
    "        'epoch': epoch,\n",
    "    }\n",
    "    torch.save(state, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch, net,trainloader,  testloader,criterion, device):\n",
    "    net.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    scores, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "\n",
    "            inputs = inputs.reshape(1,1,1,18)\n",
    "            \n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            _, outputs = net(inputs)\n",
    "            # loss = criterion(outputs, targets)\n",
    "            # test_loss += loss.item()\n",
    "            # _, predicted = outputs.max(1)\n",
    "            scores.append(outputs)\n",
    "            labels.append(targets)\n",
    "\n",
    "            # total += targets.size(0)\n",
    "            # correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            progress_bar(batch_idx, len(testloader))\n",
    "\n",
    "    # Get the prdict results.\n",
    "    scores = torch.cat(scores,dim=0).cpu().numpy()\n",
    "    labels = torch.cat(labels,dim=0).cpu().numpy()\n",
    "    scores = np.array(scores)[:, np.newaxis, :]\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # Fit the weibull distribution from training data.\n",
    "    print(\"Fittting Weibull distribution...\")\n",
    "\n",
    "    _, mavs, dists = compute_train_score_and_mavs_and_dists(train_class_num, trainloader, device, net)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1   Learning rate: 0.000010\n",
      " [===============================================================>.]  Step: 14ms | Tot: 832ms | Loss: 0.818 | Acc: 57.407% (31/54) 54/54 =====================>..........................................]  Step: 30ms | Tot: 346ms | Loss: 0.917 | Acc: 35.000% (7/20) 20/54 ===>....................]  Step: 7ms | Tot: 597ms | Loss: 0.787 | Acc: 44.737% (17/38) 38/54 ==========>..]  Step: 8ms | Tot: 817ms | Loss: 0.833 | Acc: 56.604% (30/53) 53/54 \n",
      "Saving..\n",
      " [===============================================================>.]  Step: 2ms | Tot: 202ms 36/36 .......................................................]  Step: 39ms | Tot: 43ms 3/36 ====================>.....................]  Step: 9ms | Tot: 150ms 25/36 ...............]  Step: 4ms | Tot: 164ms 28/36 \n",
      "Fittting Weibull distribution...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-82-aa1f4d72344e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;31m# plot_feature(net, trainloader, device, plotfolder, epoch=epoch,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;31m#                 plot_class_num=train_class_num, maximum=plot_max, plot_quality=plot_quality)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-81-3588a5f328c1>\u001b[0m in \u001b[0;36mtest\u001b[1;34m(epoch, net, trainloader, testloader, criterion, device)\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Fittting Weibull distribution...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmavs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdists\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_train_score_and_mavs_and_dists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_class_num\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "es = 1\n",
    "epoch=0\n",
    "plot_max = 0\n",
    "plot_quality = 200\n",
    "if not evaluate:\n",
    "    for epoch in range(start_epoch, es):\n",
    "        print('\\nEpoch: %d   Learning rate: %f' % (epoch+1, optimizer.param_groups[0]['lr']))\n",
    "        adjust_learning_rate(optimizer, epoch, lr, step=20)\n",
    "        train_loss, train_acc = train(net,trainloader,optimizer,criterion,device)\n",
    "        save_model(net, None, epoch, os.path.join(checkpoint,'last_model.pth'))\n",
    "        test_loss, test_acc = 0, 0\n",
    "        logger.append([epoch+1, optimizer.param_groups[0]['lr'], train_loss, train_acc, test_loss, test_acc])\n",
    "        # plot_feature(net, trainloader, device, plotfolder, epoch=epoch,\n",
    "        #                 plot_class_num=train_class_num, maximum=plot_max, plot_quality=plot_quality)\n",
    "        test(epoch, net, trainloader, testloader, criterion, device)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.43512735, 0.72709793, 0.7634604, 0.9856832, 1.38425899]\n",
      "1.552454219243758037905e+00 4.834796487802818454327e+00  2.538549965943251329747e+00 9.494058163839286246244e-01 2.649326762877164043175e+01 8.823093249956448502758e-01  1 5.000000000000000000000e+00 1 5 1 4.351273500000000238330e-01 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import libmr\n",
    "\n",
    "mr = libmr.MR()\n",
    "tailtofit = [0.43512735, 0.72709793, 0.7634604 , 0.9856832 , 1.38425899]\n",
    "print(tailtofit)\n",
    "mr.fit_high(tailtofit, len(tailtofit))\n",
    "print(mr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5]\n",
      "3.394289398063347729106e+00 2.293801464918516774816e+00  9.485253708554127527464e+00 1.214643368728770322917e+00 1.461669044566115616135e+01 3.599669282196622521752e-01  1 5.000000000000000000000e+00 1 5 1 1.000000000000000000000e+00 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import libmr\n",
    "\n",
    "mr = libmr.MR()\n",
    "tailtofit = [1, 2, 3 , 4 , 5]\n",
    "print(tailtofit)\n",
    "mr.fit_high(tailtofit, len(tailtofit))\n",
    "print(mr)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1f8b6ba03da8c1183842b1618cc08da02dfa809c0bdf2baf79f27c6650c5e257"
  },
  "kernelspec": {
   "display_name": "Python 3.6.12 ('pytorch1.6')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
